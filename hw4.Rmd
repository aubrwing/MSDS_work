---
title: "Homework #4: Classification" 
author: "**Your Name Here**"
date: "Due: Wed Sept 28 | 11:45am"
output: R6030::homework
---

**DS 6030 | Fall 2022 | University of Virginia**

------------------------------------------------------------------------

```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6030")) # knitr settings
options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```

::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em"}
This is an **independent assignment**. Do not discuss or work with classmates.
:::


# Required R packages and Directories

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/DS6030/data/' # data directory
library(R6030)     # functions for SYS-6030
library(tidyverse) # functions for data manipulation   
```
:::


# Crime Linkage

Crime linkage attempts to determine if two or more unsolved crimes share a common offender. *Pairwise* crime linkage is the more simple task of deciding if two crimes share a common offender; it can be considered a binary classification problem. The linkage training data has 8 evidence variables that measure the similarity between a pair of crimes:

- `spatial` is the spatial distance between the crimes
- `temporal` is the fractional time (in days) between the crimes
- `tod` and `dow` are the differences in time of day and day of week between the crimes
- `LOC`, `POA,` and `MOA` are binary with a 1 corresponding to a match (type of property, point of entry, method of entry)
- `TIMERANGE` is the time between the earliest and latest possible times the crime could have occurred (because the victim was away from the house during the crime).
- The response variable indicates if the crimes are linked ($y=1$) or unlinked ($y=0$).


These problems use the [linkage-train](https://mdporter.github.io/DS6030/data/linkage_train.csv) and [linkage-test](https://mdporter.github.io/DS6030/data/linkage_test.csv) datasets (click on links for data). 



::: {.solution}
```{r}
linkage_test <- read.csv("linkage_test.csv")
linkage_train <- read.csv("linkage_train.csv")
```

:::




# Problem 1: Penalized Regression for Crime Linkage

## a. Fit a penalized *linear regression* model to predict linkage. Use a lasso, ridge, or elasticnet penalty (your choice). 
- Report the value of $\alpha$ used (if elasticnet)
- Report the value of $\lambda$ used
- Report the estimated coefficients


::: {.solution}
I decided to use ridge regression, so my alpha value is zero. The value of lambda used was lambda min, which is 0.002327. The estimated coefficients are printed below.

```{r}
library(mlbench)
library(glmnet)
```

```{r}
#sample <- sample(c(TRUE, FALSE), nrow(linkage_train), replace=TRUE, prob=c(0.8,0.2))
#linkage_train$sample <- sample
X = glmnet::makeX(
train = linkage_train %>% select(-y)
)
X.train = X
Y.train = linkage_train %>% pull(y)
```

Estimated coefficients:
```{r}
model_ridge = cv.glmnet(X.train,Y.train,alpha=0,
                    nfolds=10, standardize=TRUE)
coef(model_ridge,s="lambda.min")
```

```{r}
model_ridge$lambda.min
```


:::


## b. Fit a penalized *logistic regression* model to predict linkage. Use a lasso, ridge, or elasticnet penalty (your choice).  
- Report the value of $\alpha$ used (if elasticnet)
- Report the value of $\lambda$ used
- Report the estimated coefficients

::: {.solution}
I also decided to use ridge regression for this problem, so my alpha value is zero. The lambda value is lambda minimum, which is 0.002327. The estimated coefficients are printed below.
```{r}
model_log_ridge = cv.glmnet(X.train,Y.train,family = "binomial",alpha=0,
                    nfolds=10, standardize=TRUE)
coef(model_log_ridge,s="lambda.min")
```

```{r}
model_log_ridge$lambda.min
```

:::


## c. Produce one plot that has the ROC curves, using the *training data*, for both models (from part a and b). Use color and/or linetype to distinguish between models and include a legend.    

::: {.solution}
#x: FPR, y=TPR
```{r}
library(yardstick)
library(precrec)
library(patchwork)
```


```{r}
gamma = predict(model_ridge, X.train, type='link')
p.hat = predict(model_ridge, X.train, type='response')

gamma2 = predict(model_log_ridge, X.train, type='link')
p.hat2 = predict(model_log_ridge, X.train, type='response')
#autoplot(ROC)
#precrec_obj <- evalmod(scores=gamma,labels=Y.train)
#precrec_obj2 <- evalmod(scores=gamma2,labels=Y.train)

#: Get performance data (by threshold)
perf = tibble(truth = Y.train, gamma, p.hat) %>%
#- group_by() + summarize() in case of ties
group_by(gamma, p.hat) %>%
summarize(n=n(), n.1=sum(truth), n.0=n-sum(truth)) %>% ungroup() %>%
#- calculate metrics
arrange(gamma) %>%
mutate(FN = cumsum(n.1), # false negatives
TN = cumsum(n.0), # true negatives
TP = sum(n.1) - FN, # true positives
FP = sum(n.0) - TN, # false positives
N = cumsum(n), # number of cases predicted to be 1
TPR = TP/sum(n.1), FPR = FP/sum(n.0)) %>%
select(-n, -n.1, -n.0, gamma, p.hat)

#: Get performance data (by threshold)
perf2 = tibble(truth = Y.train, gamma2, p.hat2) %>%
#- group_by() + summarize() in case of ties
group_by(gamma2, p.hat2) %>%
summarize(n=n(), n.1=sum(truth), n.0=n-sum(truth)) %>% ungroup() %>%
#- calculate metrics
arrange(gamma2) %>%
mutate(FN = cumsum(n.1), # false negatives
TN = cumsum(n.0), # true negatives
TP = sum(n.1) - FN, # true positives
FP = sum(n.0) - TN, # false positives
N = cumsum(n), # number of cases predicted to be 1
TPR = TP/sum(n.1), FPR = FP/sum(n.0)) %>%
select(-n, -n.1, -n.0, gamma2, p.hat2)
```


```{r}
colors <- c("Linear Regresion Model" = "blue", "Logistic Regression Model" = "red")

ggplot(perf,aes(FPR, TPR, color="Linear Regresion Model")) + geom_line() +
geom_line(data=perf2, aes(color= "Logistic Regression Model")) +
geom_abline(lty=3) +
coord_equal()+
ggtitle("ROC Curves for Logistic and Linear Regression Models") + 
labs(color="Legend",y="TPR (sensitivity)", x="FPR (1-specificity)")+
   scale_color_manual(values = colors)
```

:::


## d. Recreate the ROC curve from the penalized logistic regression model using repeated hold-out data. The following steps will guide you:
- Fix $\alpha=.75$ 
- Run the following steps 25 times:
    i. Hold out 500 observations
    ii. Use the remaining observations to estimate $\lambda$ using 10-fold CV
    iii. Predict the probability of linkage for the 500 hold-out observations
    iv. Store the predictions and hold-out labels
- Combine the results and produce the hold-out based ROC curve
- Note: by estimating $\lambda$ each iteration, we are incorporating the uncertainty present in estimating that tuning parameter. 
    
::: {.solution}
```{r}
predictionFull = c()
actual_labels = c()
#X.train = data.frame(X.train)
#Y.train = data.frame(Y.train)
for(i in 1:25){
  n.holdout = 500 # size of hold-out set
  holdout = sample(30000, size=n.holdout)
  X.holdout = X.train[-holdout,]
  Y.holdout = Y.train[-holdout]
  model = cv.glmnet(X.holdout,Y.holdout,family = "binomial",alpha=0.75,
                    nfolds=10, standardize=TRUE)
  predictions = predict(model,X.train[holdout,], type='response')
  predictionFull = c(predictionFull,predictions)
  actual_labels = c(actual_labels,Y.train[holdout])
}
```

```{r}
roc_holdout <- evalmod(scores=predictionFull,labels=actual_labels)
autoplot(roc_holdout)
```


:::




## e. Contest Part 1: Predict the estimated *probability* of linkage for the test data (using any model). 
- Submit a .csv file (ensure comma separated format) named `lastname_firstname_1.csv` that includes the column named **p** that is your estimated posterior probability. We will use automated evaluation, so the format must be exact. 
- You are free to use any tuning parameters
- You are free to use any data transformation or feature engineering
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points.     
- Your probabilities will be evaluated with respect to the mean negative Bernoulli log-likelihood (known as the average *log-loss* metric)
$$ 
L = - \frac{1}{M} \sum_{i=1}^m [y_i \log \, \hat{p}_i + (1 - y_i) \log \, (1 - \hat{p}_i)]
$$
where $M$ is the number of test observations, $\hat{p}_i$ is the prediction for the $i$th test observation, and $y_i \in \{0,1\}$ are the true test set labels. 

::: {.solution}
```{r}
X_test = glmnet::makeX(
train = linkage_test
)
p.hat = predict(model_log_ridge, X_test, type='response')
colnames(p.hat)<-c("p")
final <- write.csv(p.hat,"winger_aubrey_1.csv",row.names = FALSE,quote=FALSE)
```

:::


## f. Contest Part 2: Predict the linkages for the test data (using any model). 
- Submit a .csv file (ensure comma separated format) named `lastname_firstname_2.csv` that includes the column named **linkage** that takes the value of 1 for linkages and 0 for unlinked pairs. We will use automated evaluation, so the format must be exact. 
- You are free to use any tuning parameters.
- You are free to use any data transformation or feature engineering.
- Your labels will be evaluated based on total cost, where cost is equal to `1*FP + 8*FN`. This implies that False Negatives (FN) are 8 times as costly as False Positives (FP)    
- You will receive credit for a proper submission; the top five scores will receive 2 bonus points. Note: you only will get bonus credit for one of the two contests. 

::: {.solution}
```{r}
p.hat2 = predict(model_log_ridge, X_test,type='class')
colnames(p.hat2)<-c("linkage")
final <- write.csv(p.hat2,"winger_aubrey_2.csv",row.names = FALSE,quote=FALSE)
```

:::

    