---
title: "Homework #6: Clustering" 
author: "Aubrey Winger"
date: "Due: Wed Oct 19 | 11:45am"
output: R6030::homework
editor_options:
  chunk_output_type: console
---

**DS 6030 | Fall 2021 | University of Virginia**

------------------------------------------------------------------------

```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6030")) # knitr settings
options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```


::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em"}

This is an **independent assignment**. Do not discuss or work with classmates.

:::


# Required R packages and Directories

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/DS6030/data/' # data directory
library(R6030)     # functions for DS-6030
library(tidyverse) # functions for data manipulation   
library(mclust)    # functions for mixture models
library(mixtools)  # poisregmixEM() function
```
:::


# Problem 1: Customer Segmentation with RFM (Recency, Frequency, and Monetary Value)

RFM analysis is an approach that some businesses use to understand their customers' activities. At any point in time, a company can measure how recently a customer purchased a product (Recency), how many times they purchased a product (Frequency), and how much they have spent (Monetary Value). There are many ad-hoc attempts to segment/cluster customers based on the RFM scores (e.g., here is one based on using the customers' rank of each dimension independently: <https://joaocorreia.io/blog/rfm-analysis-increase-sales-by-segmenting-your-customers.html>). In this problem you will use the clustering methods we covered in class to segment the customers. 


The data for this problem can be found here: <`r file.path(data.dir, "RFM.csv")`>. Cluster based on the Recency, Frequency, and Monetary value columns.


::: {.solution}
```{r}
data <- read_csv("RFM.csv")
data_scaled <- c(data["id"],data[,c("Recency","Frequency","Monetary")] %>% scale() %>% as_tibble()) %>% as_tibble()
```

:::


## a. Implement hierarchical clustering. 

- Describe any pre-processing steps you took (e.g., scaling, distance metric)
- State the linkage method you used with justification. 
- Show the resulting dendrogram
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
::: {.solution}
I standard scaled the data so that the mean of each column is 0 and the sd is 1. I also removed the ID column, since this is not an actual feature of the data. I calculated euclidean distance between the three remaining features.
```{r}
id <- data["id"]
clusters <- 1000
distance_metric <- dist(data_scaled[,c("Recency","Frequency","Monetary")], method="euclidean")
hc1 = hclust(distance_metric, method="average")
plot(hc1, las=1,cex=0.6)
```

```{r}
hc2 = hclust(distance_metric, method="complete")
plot(hc2,las=1,cex=0.6)
```

```{r}
hc3 = hclust(distance_metric, method="single")
plot(hc3,las=1,cex=0.6)
```

```{r}
hc4= hclust(distance_metric, method="centroid")
plot(hc4,las=1,cex=0.6)
```

```{r}
hc5 = hclust(distance_metric, method="ward.D2")
plot(hc5,las=1,cex=0.6)
```

After graphing all the possible linkage methods, I decided to go with ward.D2 linkage. Ward's linkage had clustering that looked the most sensical, and it accounts for the number of data points in the cluster. I decided to re-plot the dendrogram with Ward's linkage in a neater fashion below.
```{r}
colPalette <- c('#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e')
clusters = cutree(hc5, k=9)
plot(as.dendrogram(hc5), las=1, leaflab="none", ylab="height")
ord = hc5$order
labels = clusters[ord]
colors = colPalette[labels]
shapes = 15 #ifelse(str_detect(labels, "F"), 15, 17)
n = length(labels)
points(1:n, rep(0, n), col=colors, pch=shapes, cex=.8)
abline(h = 1.25, lty=3, col="grey40")
```


```{r}
tibble(height = hc5$height, K = row_number(-height)) %>%
ggplot(aes(K, height)) +
geom_line() +
geom_point(aes(color = ifelse(K == 9, "red", "black"))) +
scale_color_identity() +
coord_cartesian(xlim=c(1, 50))
```

The number of segments/clusters I decided to use was k=9, since this is where there begins to be much smaller jumps in height. This means the solutions for k+1 and k clusters are becoming much more similar. k=9 is also the elbow point of the graph, so going beyond this will make the clusters much more complicated without a significant decrease in SSE.\

```{r}
print(cutree(hc5,k=9)[1])
print(cutree(hc5,k=9)[100])
```
Customers 1 and 100 are not in the same cluster under this segmentation.
:::



## b. Implement k-means.  

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Using your segmentation, are customers 1 and 100 in the same cluster?     
    
::: {.solution}
I standard scaled the data so that the mean of each column is 0 and the standard deviation is 1. I also removed the ID column, since this is not an actual feature of the data. The best number of clusters is k=4, because this is the closest approximation to where the elbow of the plot is that compares SSE and k. Increasing the k value will only increase the complexity of the model without decreasing the SSE significantly. Using this segmention, customers 1 and 100 are not in the same cluster.
```{r}
Kmax=10
SSE=numeric(Kmax)
set.seed(2022)
for(k in 1:Kmax){
  km = kmeans(data_scaled[,c("Recency","Frequency","Monetary")],centers=k,nstart=100)
  SSE[k]=km$tot.withinss
}
```

```{r}
tibble(K = 1:Kmax, SSE) %>%
ggplot(aes(K, SSE)) + geom_line() + geom_point() +
scale_x_continuous(breaks = 1:Kmax) +
labs(title = "K-means for Customer Segmentation")
```

```{r}
library(broom)
library(tidyselect)
```


```{r}
fit = kmeans(data_scaled[,c("Recency","Frequency","Monetary")],centers=4,nstart=100)
data_scaled_cluster <- augment(fit,data_scaled)
```

```{r}
print(data_scaled_cluster[data_scaled_cluster$id==1,])
print(data_scaled_cluster[data_scaled_cluster$id==100,])
```
:::

## c. Implement model-based clustering

- Describe any pre-processing steps you took (e.g., scaling)
- State the number of segments/clusters you used with justification. 
- Describe the best model. What restrictions are on the shape of the components?
- Using your segmentation, are customers 1 and 100 in the same cluster?     

::: {.solution}
I standard scaled the data to have a mean of 1 and a standard deviation of 0. The number of clusters found by the Guassian model based clustering/Mclust VVE is 9. This is the number of clusters to use that maximizes the BIC. The best model is a VVE model (Mclust VVE (ellipsoidal, equal orientation)) with 9 clusters, a BIC of -66610, log likelihood of -33006, degrees of freedom of 65, and 10,000 observations. The restrictions on the shape of the components is that they are ellipsoidal and equal orientation. Using this segmentation, customers 1 and 100 are not in the same cluster.
```{r}
set.seed(2002)
mix = Mclust(data_scaled[,c("Recency","Frequency","Monetary")],verbose=FALSE)
summary(mix)
```

```{r}
glance(mix)
```

```{r}
data_scaled_mclust <- augment(mix, data_scaled)
print(data_scaled_mclust[data_scaled_mclust$id==1,])
print(data_scaled_mclust[data_scaled_mclust$id==100,])
```

```{r}
plot(mix,what="BIC")
```



:::

## d. Discuss how you would cluster the customers if you had to do this for your job. Do you think one model would do better than the others? 

::: {.solution}
For my job, I would cluster the customers in 9 clusters since two of my models showed that this was the best k value. I think the model-based clustering would do better than the others. Hierarchical clustering requires a person to interpret the dendrogram, so the number of clusters is determined by you. This might not be as accurate as a optimization algorithm such as mclust. Since k uses random initialized centroids, it can often have variable results. Additionally, it does not optimize itself so the user has to pick the number of clusters. On the other hand, model based clustering uses soft assignment, so the decision boundaries are more flexible than they are in k means. Although mixture models take longer to converge than k-means, overall it is a more robust algorithm so I think it is the best.
:::



# Problem 2: Poisson Mixture Model

The pmf of a Poisson random variable is:
\begin{align*}
f_k(x; \lambda_k) = \frac{\lambda_k^x e^{-\lambda_k}}{x!}
\end{align*}

A two-component Poisson mixture model can be written:
\begin{align*}
f(x; \theta) = \pi \frac{\lambda_1^x e^{-\lambda_1}}{x!} + (1-\pi) \frac{\lambda_2^x e^{-\lambda_2}}{x!}
\end{align*}



## a. What are the parameters of the model? 

::: {.solution}
The parameters of the model are x and theta, where theta is equal to pi, lambda_1, lambda_2. There are four parameters in the model.
:::

## b. Write down the log-likelihood for $n$ independent observations ($x_1, x_2, \ldots, x_n$). 

::: {.solution}
This solution is in the attached pdf!
:::

## c. Suppose we have initial values of the parameters. Write down the equation for updating the *responsibilities*. 

::: {.solution}
The solution is in the attached pdf!
:::


## d. Suppose we have responsibilities, $r_{ik}$ for all $i=1, 2, \ldots, n$ and $k=1,2$. Write down the equations for updating the parameters. 

::: {.solution}
The solution is in the attached pdf!
:::



## e. Fit a two-component Poisson mixture model, report the estimated parameter values, and show a plot of the estimated mixture pmf for the following data:

```{r, echo=TRUE}
#-- Run this code to generate the data
set.seed(123)             # set seed for reproducibility
n = 200                   # sample size
z = sample(1:2, size=n, replace=TRUE, prob=c(.25, .75)) # sample the latent class
theta = c(8, 16)          # true parameters
y = ifelse(z==1, rpois(n, lambda=theta[1]), rpois(n, lambda=theta[2]))
```


- Note: The function `poisregmixEM()` in the R package `mixtools` is designed to estimate a mixture of *Poisson regression* models. We can still use this function for our problem of pmf estimation if it is recast as an intercept-only regression. To do so, set the $x$ argument (predictors) to `x = rep(1, length(y))` and `addintercept = FALSE`. 
    - Look carefully at the output from this model. The `beta` values (regression coefficients) are on the log scale.


::: {.solution}
The parameters are lambda = 0.272, 0.782 and beta =exp(2.06), exp(2.78). The log likelihood is -611.2.
```{r}
model <- poisregmixEM(y=y,x=rep(1,length(y)),addintercept=FALSE,verb=FALSE)
```

```{r}
summary(model)
```


```{r}
hist(model$posterior, breaks=50,probability=TRUE,xlab="Posterior Probability",main="Estimated Mixture PMF")
```

```{r}
plot(density(model$posterior),main="Estimated Mixture PMF",xlab="Posterior Probability")
```



:::


## f. **2 pts Extra Credit**: Write a function that estimates this two-component Poisson mixture model using the EM approach. Show that it gives the same result as part *e*. 
- Note: you are not permitted to copy code.  Write everything from scratch and use comments to indicate how the code works (e.g., the E-step, M-step, initialization strategy, and convergence should be clear). 
- Cite any resources you consulted to help with the coding. 


::: {.solution}
ADD SOLUTION HERE
:::


